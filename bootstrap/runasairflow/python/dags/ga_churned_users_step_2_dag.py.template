import datetime
from airflow.models import DAG
from airflow.operators.bash_operator import BashOperator

args = { 'owner': 'airflow',
         'start_date': STEP_2_START_DATE_AS_PY_CODE,
         'retries': 16,
         'retry_delay': datetime.timedelta(minutes=30) }

dag = DAG(dag_id='ga_churned_users_step_2',
          default_args=args,
          schedule_interval='@weekly')

try:
    with open('/tmp/today_as_str.txt', 'r') as f:
        today_as_str = f.read().strip()
except:
    today_as_str = ''

try:
    with open('/tmp/unique_hash.txt', 'r') as f:
        unique_hash = f.read().strip()
except:
    unique_hash = ''

# Do not remove the extra space at the end (the one after 'runpysparkpreprocessor.sh')
task_3_run_pyspark_preprocessor_cmd_parts = [
    f'TODAY_AS_STR={today_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'TRAINING_OR_PREDICTION=training',
    'docker run --rm',
    '-v /opt/samplecode:/opt/samplecode',
    '-e ENVIRONMENT_TYPE',
    '-e TODAY_AS_STR',
    '-e UNIQUE_HASH',
    '-e TRAINING_OR_PREDICTION',
    '-e MORPHL_SERVER_IP_ADDRESS',
    '-e MORPHL_CASSANDRA_USERNAME',
    '-e MORPHL_CASSANDRA_KEYSPACE',
    '-e MORPHL_CASSANDRA_PASSWORD',
    'pysparkcontainer',
    'bash /opt/samplecode/python/pyspark/runpysparkpreprocessor.sh ']
task_3_run_pyspark_preprocessor_cmd = ' '.join(task_3_run_pyspark_preprocessor_cmd_parts)

# Do not remove the extra space at the end (the one after 'move_metadata.sh')
task_4_move_metadata_cmd_parts = [
    f'TODAY_AS_STR={today_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'bash /opt/orchestrator/bootstrap/runasairflow/bash/move_metadata.sh ']
task_4_move_metadata_cmd = ' '.join(task_4_move_metadata_cmd_parts)

# Do not remove the extra space at the end (the one after 'rundaskpreprocessor.sh')
task_5_run_dask_preprocessor_cmd_parts = [
    f'TODAY_AS_STR={today_as_str}',
    f'UNIQUE_HASH={unique_hash}',
    'TRAINING_OR_PREDICTION=training',
    'MODELS_DIR=/opt/models',
    'docker run --rm',
    '-v /opt/samplecode:/opt/samplecode',
    '-e ENVIRONMENT_TYPE',
    '-e TODAY_AS_STR',
    '-e UNIQUE_HASH',
    '-e TRAINING_OR_PREDICTION',
    '-e MODELS_DIR',
    '-e MORPHL_SERVER_IP_ADDRESS',
    'pysparkcontainer',
    'bash /opt/scaler/rundaskpreprocessor.sh ']
task_5_run_dask_preprocessor_cmd = ' '.join(task_5_run_dask_preprocessor_cmd_parts)

# Do not remove the extra space at the end (the one after 'truncate_tables_for_step_2.sh')
task_1_truncate_tables = BashOperator(
    task_id='task_1_truncate_tables',
    bash_command='bash /opt/orchestrator/bootstrap/runasairflow/bash/truncate_tables_for_step_2.sh ',
    dag=dag)

# Do not remove the extra space at the end (the one after 'generate_id_files.sh')
task_2_generate_id_files = BashOperator(
    task_id='task_2_generate_id_files',
    bash_command='bash /opt/orchestrator/bootstrap/runasairflow/bash/generate_id_files.sh ',
    dag=dag)

task_3_run_pyspark_preprocessor = BashOperator(
    task_id='task_3_run_pyspark_preprocessor',
    bash_command=task_3_run_pyspark_preprocessor_cmd,
    dag=dag)

task_4_move_metadata = BashOperator(
    task_id='task_4_move_metadata',
    bash_command=task_4_move_metadata_cmd,
    dag=dag)

task_5_run_dask_preprocessor = BashOperator(
    task_id='task_5_run_dask_preprocessor',
    bash_command=task_5_run_dask_preprocessor_cmd,
    dag=dag)

task_2_generate_id_files.set_upstream(task_1_truncate_tables)
task_3_run_pyspark_preprocessor.set_upstream(task_2_generate_id_files)
task_4_move_metadata.set_upstream(task_3_run_pyspark_preprocessor)
task_5_run_dask_preprocessor.set_upstream(task_4_move_metadata)
